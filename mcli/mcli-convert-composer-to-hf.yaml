integrations:
- integration_type: git_repo
  git_repo: ruichenmle/ava-llm-foundry
  git_branch: sut-mpt7b-8k-finetune # use your branch
  pip_install: -e .[gpu]
  ssh_clone: true # Should be true if using a private repo

# We are fetching, converting, and training on the 'val' split
# as it is small and quick to get going for this demo.
# For real training runs, follow the instructions in `llm-foundry/scripts/train/README.md`
# to convert and host the full 'train' dataset.

command: |
  cd ava-llm-foundry/scripts/
  python inference/convert_composer_to_hf.py \
    --composer_path s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/avalaraGPT/model/mpt-7b-8k/raw-sut-finetuned/mono/ep10-ba80.pt \
    --hf_output_path s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/avalaraGPT/model/mpt-7b-8k/raw-sut-finetuned-hf/ \
    --output_precision bf16

image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04
# Mosaic Cloud will use run_name (with a unique suffix) to populate the env var $COMPOSER_RUN_NAME

gpu_num: 8
gpu_type: a100_40gb
cluster: r8z3 # replace with your cluster here!


# Run Name
run_name: mpt-7b-8k-raw-sut-finetuned-model-conversion # If left blank, will be read from env var $RUN_NAME
